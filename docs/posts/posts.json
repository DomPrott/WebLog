[
  {
    "path": "posts/05.05.23_PairsTrading/",
    "title": "Algo Trading with a Cointegrated Pairs Trading Strategy",
    "description": "Cointegration is a valuable statistical tool in univariate pairs trading as it helps to identify the hedge ratio between two related assets and profit from their historical relationship. This approach can lead to a more balanced and risk-controlled portfolio, allowing traders to profit from market fluctuations.",
    "author": [
      {
        "name": "Dominic Prott",
        "url": {}
      }
    ],
    "date": "2023-04-12",
    "categories": [
      "Stocks",
      "R"
    ],
    "contents": "\r\nIntroduction\r\nThe Cointegrated Augmented Dickey-Fuller (CADF) approach is a popular trading strategy used in the domain of short-term mean-reversion models. The aim of CADF is to generate profits from the long-term relationship between stocks that are closely linked by buying and selling them based on extreme deviations from their historical relationship. CADF is a form of pairs trading, where two stocks are cointegrated and traded based on their price differences.\r\nIn this article, we will explore the basics of CADF and examine its practical application. We will begin by simulating stock prices to illustrate the concept of cointegration and demonstrate how it can be used to identify suitable pairs of stocks for pairs trading. Next, we will implement the pairs trading strategy using real stock pairs and assess its profitability through backtesting. This will involve testing the strategy on historical data to determine profit and loss, enabling us to make informed decisions about its suitability for investment goals.\r\nCointegration Simulation\r\nAccording to the random walk theory, stock prices can be modeled as random walks. Therefore, we start with simulating a simple random walk which then will act as the underlying stochastic trend for our stock pair:\r\n\\[\r\n\\begin{split}\r\nz_t =& \\ z_{t-1} + e_t \\\\\r\n=& \\ z_{t-2} + e_{t-1} + e_t \\\\\r\n=& \\ ... \\\\\r\n=& \\ z_0 + \\sum_{j=0}^{T-1} e_{t-j}\r\n\\end{split}\r\n\\]\r\nwhere \\(e_t \\sim WN(0, \\sigma^2)\\) is a Gaussian white noise error term and \\(t=1,2,..., T\\). The intercept term will be set to 0, thus vanishing in the following. We subsequently use this series \\(z_t\\) to create the cointegrated stock pair.\r\n\r\n\r\nN <- 1000\r\n\r\nset.seed(123)\r\nz <- cumsum(rnorm(N, mean = 0, sd = 4))\r\n\r\nsigma_x <- 1.5\r\nsigma_y <- 1.8\r\n\r\ny0 <- 60\r\nx0 <- 50\r\n\r\na <- 0.4\r\nb <- 0.6\r\n\r\ny <- y0 + a*z + rnorm(N, mean = 0, sd = sigma_y)\r\nx <- x0 + b*z + rnorm(N, mean = 0, sd = sigma_x)\r\n\r\nlibrary(ggplot2)\r\nlibrary(gridExtra)\r\n\r\ngrid.arrange(\r\nggplot() +\r\n  geom_line(aes(x = 1:N, y = z), col = \"blue\") +\r\n  theme_minimal() +\r\n  labs(x = NULL, y = NULL),\r\nggplot() + \r\n  geom_line(aes(x = 1:N, y = y, col = \"y\")) +\r\n  geom_line(aes(x = 1:N, y = x, col = \"x\")) +\r\n  theme_minimal() +\r\n  labs(color = \"Series\", x = NULL, y = NULL),\r\nnrow = 2)\r\n\r\n\r\n\r\nThe two “stock prices” below are descendants of the random walk at the top, only deviating in their intercept terms \\(y_0\\) and \\(x_0\\), the coefficients \\(a\\) and \\(b\\) and their respective variances \\(\\sigma_y^2\\) and \\(\\sigma_x^2\\).\r\nThe processes are said to be cointegrated when there exists a linear combination of them that is stationary over time. Such a linear combination might be the spread between two stock prices as in this case. When the statistical properties of the spread - their mean and variance - remain constant over time, it is said to be stationary or mean-reverting making it forecastable.\r\n\\[\r\n\\begin{split}\r\nspread_t &= y_t - \\gamma x_t \\\\\r\n&= (y_0- \\gamma x_0) + (az_t - \\gamma bz_{t}) + (e_{yt}- \\gamma e_{xt})\r\n\\end{split}\r\n\\]\r\nThe series are cointegrated and \\(spread_t\\) is stationary for \\((az_t - \\gamma bz_{t}) = 0\\) which implies \\((a - \\gamma b)=0\\). Since \\(a=0.4\\) and \\(b=0.6\\), this is the case for when the hedge ratio \\(\\gamma = 2/3\\). It can also be written as:\r\n\\[\r\n\\begin{split}\r\nspread_t &= y_t - \\gamma x_t \\\\\r\n&= \\mu + e_t\r\n\\end{split}\r\n\\]\r\nThe spread is constructed to be stationary, because by subtracting one stock price from the other, their common trends are removed. This leaves behind only the white noise error term, denoted by \\(e_t\\), which has a mean of 0 and a variance of \\(\\sigma^2\\). It is stationary, allowing for statistical analysis and modeling of the relationship between the two stocks which can furthermore be expressed in terms of a linear regression model of the form:\r\n\\[\r\ny_t = \\mu + \\gamma x_t + e_t\r\n\\]\r\nSince we do not know about cointegration in real world examples, we make use of the Two-Step-Engle-Granger-Procedure. This involves modelling the relationship in the form of a linear regression model, estimating it and then testing for the stationarity property of its residuals using unit root tests like the Augmented Dickey-Fuller (adf-test). This adf-test is applied next on \\(e_t\\):\r\n\r\n\r\nmodel1 <- lm(y~x)\r\nmodel2 <- lm(x~y)\r\n\r\nmodel1\r\n\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nCoefficients:\r\n(Intercept)            x  \r\n    27.2351       0.6598  \r\n\r\nmodel2\r\n\r\n\r\nCall:\r\nlm(formula = x ~ y)\r\n\r\nCoefficients:\r\n(Intercept)            y  \r\n    -38.009        1.471  \r\n\r\nlibrary(urca)\r\nsummary(ur.df(model1$residuals, selectlags = \"BIC\"))\r\n\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.6407 -1.3675 -0.0546  1.4070  5.8747 \r\n\r\nCoefficients:\r\n           Estimate Std. Error t value Pr(>|t|)    \r\nz.lag.1    -1.07032    0.04539 -23.582   <2e-16 ***\r\nz.diff.lag  0.03979    0.03164   1.258    0.209    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.036 on 996 degrees of freedom\r\nMultiple R-squared:  0.5156,    Adjusted R-squared:  0.5147 \r\nF-statistic: 530.2 on 2 and 996 DF,  p-value: < 2.2e-16\r\n\r\n\r\nValue of test-statistic is: -23.5816 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau1 -2.58 -1.95 -1.62\r\n\r\ngrid.arrange(\r\n    ggplot() + \r\n      geom_line(aes(x = 1:N, y = model1$residuals)) +\r\n      theme_minimal() +\r\n      labs(color = \"Series\", x = NULL, y = NULL),\r\n    ggplot() + \r\n      geom_line(aes(x = 1:N, y = model2$residuals)) +\r\n      theme_minimal() +\r\n      labs(color = \"Series\", x = NULL, y = NULL), \r\n  nrow = 2)\r\n\r\n\r\n\r\nAs we can see from the estimation, the hedge ratio \\(\\gamma = 0.67\\) is close enough to conclude on \\((a-\\gamma b) = 0\\) such that there are only the stationary residuals left. In the subsequent adf-test the null hypothesis of non-stationarity is rejected at 1% significance level which is not surprising as the simulated processes were created in such a way in the first place. Moreover, the graphical representation of the two residual series shows mean reversion for both around 0 and similar amplitudes.\r\nTherefore, we can conclude that the two simulated series are cointegrated and that their spread reverts back to its mean with a constant variance making it possible to forecast it. We should be able to make profits from this by buying and selling the two stock prices based on their spread and converging behavior.\r\nFind Pairs\r\nTo implement CADF, the initial challenge is to identify stocks with a long-term bidirectional relationship. This can be achieved by looking for same industry/sector stocks with similar market capitalization or pairing ETFs of related products. Alternatively, A and B share stocks like Berkshire Hathaway can be considered. Next, we estimate the spread using static OLS. In the future, means of rolling estimation like Kalman filtering can be applied to achieve more accurate results.\r\nCointegration Test\r\nLike in Ernest Chan, 2013, the pair to be examined are ETF’s which reflect the commodity-based economies of Australia and Canada (EWA and EWC respectively). At first, we logarithmize the stock prices to help normalize the data, then chart their series and visually inspect their relationship.\r\n\r\n\r\nlibrary(tidyquant)\r\n\r\nEWA <- tq_get(\"EWA\", get = \"stock.prices\", from = \"2000-01-01\", to = today()) %>% \r\n            select(date, adjusted)\r\n  \r\nEWC <- tq_get(\"EWC\", get = \"stock.prices\", from = \"2000-01-01\", to = today()) %>% \r\n            select(date, adjusted)\r\n  \r\nseries <- merge(EWA, EWC, by = \"date\")\r\nnames(series) <- c(\"date\", \"EWA\", \"EWC\")\r\nseries$EWA <- log(series$EWA)\r\nseries$EWC <- log(series$EWC)\r\n\r\ngrid.arrange(\r\nseries %>% \r\n  ggplot() +\r\n    geom_line(aes(x = date, y = EWA, col = \"EWA\")) +\r\n    geom_line(aes(x = date, y = EWC, col = \"EWC\")) +\r\n    theme_minimal() +\r\n    labs(color = \"Series\", x = NULL, y = NULL),\r\nseries %>% \r\n  ggplot() +\r\n  geom_point(aes(x = EWA, y = EWC)) + \r\n  theme_minimal() +\r\n  labs(x = paste(colnames(series)[2], \"Adjusted Price in USD\"), y = paste(colnames(series)[3], \"Adjusted Price in USD\")),\r\nnrow = 2)\r\n\r\n\r\n\r\nFrom the look of it, it seems they follow similar patterns. With the exception of the cluster near the origin, the scatter plot is relatively diagonal which is also indicative for common price movements. Similar to the simulation, we apply OLS to a linear regression model, next. However, we first split the data into training and test periods to compare the results and the model’s accuracy later on.\r\n\r\n\r\ntrain <- round(0.7*nrow(series)) #split according to 70:30 rule\r\ntest <- nrow(series) - train\r\n\r\nmodel1 <- lm(EWA~EWC, data = series[1:train,])\r\nmodel2 <- lm(EWC~EWA, data = series[1:train,])\r\n\r\ncoef(model1)\r\n\r\n(Intercept)         EWC \r\n  -1.158870    1.226874 \r\n\r\ncoef(model2)\r\n\r\n(Intercept)         EWA \r\n   1.024841    0.778454 \r\n\r\nAccording to the Engle-Granger_Procedure we test the residuals of the models for stationarity using the adf-unit-root-test:\r\n\r\n\r\nresiduals_df <- data.frame(date = series$date,\r\n                           residuals1 = series$EWA - (coef(model1)[1] + coef(model1)[2]*series$EWC),\r\n                           residuals2 = series$EWC - (coef(model2)[1] + coef(model2)[2]*series$EWA))\r\n\r\nresiduals_df %>% \r\n  ggplot() + \r\n    geom_line(aes(x = date, y = residuals1)) +\r\n    theme_minimal() +\r\n    labs(x = NULL, y = NULL) +\r\n    geom_hline(yintercept = mean(residuals_df$residuals1), col = \"blue\")\r\n\r\n\r\nur.df(residuals_df$residuals1, selectlags = \"BIC\")\r\n\r\n\r\n############################################################### \r\n# Augmented Dickey-Fuller Test Unit Root / Cointegration Test # \r\n############################################################### \r\n\r\nThe value of the test statistic is: -3.7346 \r\n\r\nur.df(residuals_df$residuals2, selectlags = \"BIC\")\r\n\r\n\r\n############################################################### \r\n# Augmented Dickey-Fuller Test Unit Root / Cointegration Test # \r\n############################################################### \r\n\r\nThe value of the test statistic is: -3.7404 \r\n\r\nBoth models reject the null hypothesis of non-stationarity at conventional significance levels. The critical values are the same as in the simulated case above.\r\nTo determine which series to select as dependent and independent variable, we will rely on the value of the adf test statistic. The more negative the result, the less uncertainty about the stationarity of the residuals. In this case, model 2 reveals to be a slightly better fit. Thus, EWC as the dependent variable is treated as the primary symbol.\r\nTrade Signals\r\nSince we can now be arbitrarily sure that the series are cointegrated, we continue with creating trade signals according to our long-short-strategy. For that, we standardize the spread treating it as a Z-Score which is defined as:\r\n\r\n\r\nresiduals <- residuals_df$residuals2\r\nresiduals_df$z_score <- (residuals-mean(residuals))/sd(residuals) \r\n\r\nresiduals_df %>% \r\n  ggplot() + \r\n    geom_line(aes(x = date, y = z_score)) +\r\n    theme_minimal() +\r\n    labs(x = NULL, y = NULL) +\r\n    geom_hline(yintercept = -1.6, linetype = \"dashed\", col = \"green\") + \r\n    geom_hline(yintercept = 1.6, linetype = \"dashed\", col = \"green\") +\r\n    geom_hline(yintercept = -2.4, linetype = \"dashed\", col = \"red\") + \r\n    geom_hline(yintercept = 2.4, linetype = \"dashed\", col = \"red\") +\r\n    geom_vline(xintercept = series$date[train], col = \"blue\") +\r\n    geom_hline(yintercept = mean(residuals), col = \"blue\")\r\n\r\n\r\n\r\nThe vertical blue line represents the end of the training period, here and the horizontal one the mean for better visual orientation. The dashed green line represents trading signals. The trading strategy could moreover include a stop-loss which is displayed by the dashed red line.\r\nWarning: With standardization we made the assumption that the residual series now is standard normal distributed. The following code examines whether this assumption is to hold.\r\n\r\n\r\nhist(residuals_df$z_score)\r\n\r\n\r\nlibrary(qqplotr) #add for confidence interval\r\nggplot(residuals_df, aes(sample = z_score)) +\r\n  stat_qq() +\r\n  stat_qq_line() +\r\n  stat_qq_band()\r\n\r\n\r\n#Shapiro test for normality limited to 5000 obs\r\nset.seed(123)\r\nsample_z <- sample(residuals_df$z_score, size = 500)\r\nshapiro.test(sample_z)\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  sample_z\r\nW = 0.93882, p-value = 1.796e-13\r\n\r\nFor the sake of the argument, we assume the residuals to be normally distributed in the following, even though they appear not to be in all three checks. When applying the CADF pairs trading strategy, make sure your assumptions are met beforehand for optimal results.\r\nThe trade signals are the values 1 for long-position, -1 for short-position and 0 for no position. They are generated using a function which assigns the values based on the movements of the Z-Score in relation to predetermined long and short thresholds. The long-threshold is selected to be -1.6 and the short threshold as equal to 1.6.\r\n\r\n\r\nz_score <- residuals_df$z_score\r\n\r\nlong_signal = rep(-1.6, length(z_score)) #equal to yintercept in previous graph\r\nshort_signal = rep(1.6, length(z_score))\r\n\r\ngenerate_signal <- function(z_score, long_signal, short_signal) {\r\n  signal <- rep(0,length(z_score))\r\n  \r\n  #initialize\r\n  signal[1] <- 0\r\n  if (z_score[1] <= long_signal[1]) {\r\n    signal[1] <- 1\r\n  } else if (z_score[1] >= short_signal[1])\r\n    signal[1] <- -1\r\n  \r\n  for (t in 2:length(z_score)) {\r\n    if (signal[t-1] == 0) {  \r\n      if (z_score[t] <= long_signal[t]) {\r\n        signal[t] <- 1\r\n      } else if(z_score[t] >= short_signal[t]) {\r\n        signal[t] <- -1\r\n      } else signal[t] <- 0\r\n    } else if (signal[t-1] == 1) { \r\n      if (z_score[t] >= 0) signal[t] <- 0\r\n      else signal[t] <- signal[t-1]\r\n    } else { \r\n      if (z_score[t] <= 0) signal[t] <- 0\r\n      else signal[t] <- signal[t-1]\r\n    }\r\n  }\r\n  return(signal)\r\n}\r\n\r\n\r\nThe returned vector \\(signal\\) now only consists of [-1,0,1]. In the following, it is plotted alongside the Z-Score and the dashed long and short thresholds:\r\n\r\n\r\nresiduals_df$signal <- generate_signal(z_score, long_signal, short_signal)\r\n\r\nresiduals_df %>% \r\n  ggplot() + \r\n    geom_line(aes(x = date, y = signal, col = \"Signal\")) +\r\n    geom_line(aes(x = date, y = z_score, col = \"Z-Score\")) +\r\n    geom_hline(yintercept = -1.6, linetype = \"dashed\", col = \"green\") + \r\n    geom_hline(yintercept = 1.6, linetype = \"dashed\", col = \"green\") +\r\n    geom_vline(xintercept = series$date[train], col = \"blue\") +\r\n    theme_minimal() +\r\n    labs(color = NULL, x = NULL, y = NULL) \r\n\r\n\r\n\r\nProfit and Loss\r\nThe investment goal is to maximize profit from our pairs trading strategy, so at last we will examine the daily profit and loss (P&L) as a backtest for on this long-short portfolio. A simple way of doing this is taking the Z-Score of the historic price data of our two series as well as the trading signal and then calculating cumulative daily returns. As a quick recap, the calculation implicitly present throughout the code up to this point and next is then in summary:\r\nConstruct portfolio as residuals of a linear regression with \\(\\gamma\\) as hedge ratio: \\(e_t = y_t - \\gamma x_t\\)\r\nGet the trade signals and the resulting position \\(P_t\\) from our strategy\r\nGet daily returns: \\(R_t = e_t - e_{t-1}\\)\r\nCompute daily returns \\(R_t^s\\) from our strategy with respect to the positions: \\(R_t^s = R_tP_{t-1}\\)\r\nCompute cumulative daily P&L from equity curve: \\(\\epsilon_t = \\sum_0^T R_t^s\\)\r\n\r\n\r\nspread_return <- diff(residuals) #daily returns\r\nresiduals_df$traded_return <- spread_return * lag(residuals_df$signal)\r\n\r\nresiduals_df <- residuals_df %>%  \r\n                mutate(traded_return = ifelse(is.na(traded_return), 0, traded_return))\r\n\r\n\r\nThe daily returns from our trades look like this:\r\n\r\n\r\nresiduals_df %>% \r\n  ggplot() +\r\n    geom_line(aes(x = date, y = traded_return)) +\r\n    geom_vline(xintercept = series$date[train], col = \"blue\") +\r\n    theme_minimal() +\r\n    labs(color = NULL, x = NULL, y = NULL) \r\n\r\n\r\n\r\nAnd the P&L equity curve with percentage returns on the y-axis due to logarithmizing looks like this:\r\n\r\n\r\nresiduals_df$no_reinvest <- 1 + cumsum(residuals_df$traded_return)\r\nresiduals_df$with_reinvest <- cumprod(1 + residuals_df$traded_return)\r\n\r\ngrid.arrange(\r\n  residuals_df %>% \r\n    ggplot() +\r\n      geom_line(aes(x = date, y = no_reinvest)) +\r\n      geom_vline(xintercept = series$date[train], col = \"blue\") +\r\n      theme_minimal() +\r\n      labs(color = NULL, x = NULL, y = NULL),\r\n  residuals_df %>% \r\n    ggplot() +\r\n      geom_line(aes(x = date, y = with_reinvest)) +\r\n      geom_vline(xintercept = series$date[train], col = \"blue\") +\r\n      theme_minimal() +\r\n      labs(color = NULL, x = NULL, y = NULL),\r\n  nrow = 2)\r\n\r\n\r\n\r\nConclusion\r\nWhen concluding, we must observe that this strategy does indeed not yield relevant returns over a more than 20 years period. There are however some clues on why that is. Firstly, there could be found more profitable cointegrated series. Secondly, the long-short strategy with daily trade closings is not suitable. Thirdly and most obviously, the static OLS linear model is not a good fit to ever changing values of non-stationary stock prices. Means of rolling cointegration and rolling least squares or Kalman filtering could improve the strategy drastically.\r\nReferences\r\nPei, Hansen, The Correct Vectorized Backtest Methodology for Pairs Trading, in: https://hudsonthames.org/correct-backtest-methodology-pairs-trading/ (last visited: 12.04.23)\r\nQuantstart, Cointegrated Augmented Dickey Fuller Test for Pairs Trading Evaluation in R, in: https://www.quantstart.com/articles/Cointegrated-Augmented-Dickey-Fuller-Test-for-Pairs-Trading-Evaluation-in-R/ (last visited 12.04.23)\r\nPalomar, Daniel P., Pairs Trading with R, in: https://palomar.home.ece.ust.hk/MAFS5310_lectures/Rsession_pairs_trading_with_R.html#Trading_the_spread (last visited 12.04.23)\r\nAlexandre Rubesam, Pairs Trading: Replicating Gatev, Goetzmann and Rouwenhorst (2006), in: https://rpubs.com/arubesam/ReplicatingGGR (last visited 12.04.23)\r\n\r\n\r\n\r\n",
    "preview": "posts/05.05.23_PairsTrading/05.05.23_PairsTrading_files/figure-html5/rw-1.png",
    "last_modified": "2023-05-05T11:17:00+02:00",
    "input_file": "05.05.23_PairsTrading.knit.md"
  },
  {
    "path": "posts/05.05.23_StockValuationPt.2/",
    "title": "Stock Valuation via R Shiny, Part II: Forecasting Stock Prices",
    "description": "Predicting pstock prices can reduce uncertainty in investment decisions and provide valuable insight into market trends. While it helps investors making more informed decisions, this task is riddled with difficulties due to the nature of price movements. This post focuses on ARIMA models and Facebook's Prophet for stock price forecasting as a feature to my stock valuation app.",
    "author": [
      {
        "name": "Dominic Prott",
        "url": {}
      }
    ],
    "date": "2023-03-24",
    "categories": [
      "Shiny",
      "Stocks",
      "R"
    ],
    "contents": "\r\nIntroduction\r\nAccurately forecasting price movements is the ultimate goal of stock analysis. However, due to the volatile and nonlinear nature of stock markets, predicting price actions remains a challenge. This is further complicated by various macro and micro factors such as market sentiment, global economic conditions, political events and the company’s performance. The efficient market hypothesis even suggests that it is practically impossible to predict asset prices, as they already convey all currently available information.\r\nStill, for reasons explained next, I will show how I integrated ARIMA models and Prophet into my stock valuation app and provide a brief introduction to these methods.\r\nReasoning\r\nWhile it may not be possible to accurately predict stock prices, advancements in technology and computing power have made it easier to process and analyze large amounts of financial data. This has allowed for the development of more sophisticated modeling techniques, such as ARIMA and machine learning algorithms, that can identify patterns and relationships in the data that may not be apparent to the human eye. By leveraging these techniques, investors can reduce uncertainty regarding investment decisions and gain a deeper understanding of market trends. Contrary to what the efficient market hypothesis suggests, high-ranking investors have repeatedly shown that it is indeed possible to outperform the market by taking advantage of its imperfections which are rooted in the cognitive bias of the participants.\r\nUnder this impression, advanced methods such as ARIMA models and Facebook’s Prophet can be employed to create more reliable forecasts than what is obtained by simple or exponential moving averages.\r\nUI\r\n\r\nThe main panel of the dashboard features at the top the four info boxes, as described in the initial part of the series. Underneath, users can access either the ARIMA or Prophet model output, which is dependent on the respective tab selection. The output consists of a chart and a table displaying the projected values. On default, the chart is zoomed in for greater detail, but users can view the entire time series by double-clicking on the chart. Both forecasting methods use the whole range of data available on yahoo finance.\r\nThe main panel includes a useful feature for comparing forecast performance with actual values: the ex-post out of sample forecast. Since the accuracy of Prophets forecasts can be assessed in the graph already, this metric is only available for the ARIMA model. Thereby, an ARIMA specification is trained on a designed train set where the underlying stock price data is truncated by the specified forecasting horizon \\(h\\). In this way, users can assess the reliability of the models on a test set, allowing them to see how the model would have performed if it had been deployed \\(h\\) periods ago. This feature provides valuable insight into the accuracy of the model with its metrics being shown in detail in the box right next to it.\r\nARIMA\r\n\r\nA cornerstone of empirical time series analysis is accurate forecasting to make informed predictions about future values based on past trends. The ARIMA model is widely recognized as one of the most established and effective techniques for achieving this. It consists of three components:\r\nAn Auto Regressive (AR(p)) part of order p which regresses on its own past or lagged values to predict future ones\r\nAn Integrated (I(d)) part of order d which eliminates non-stationary processes by differencing\r\nA Moving Average (MA(q)) part of order q which attempts to predict future values based on prior forecasting errors (not to be confused with conventional moving averages in trend analysis)\r\nThe merging of these components results in an ARIMA(p,d,q) model. For better understanding, have a look at the baseline ARMA(p,q) model:\r\n\\[\r\n\\hat{y_t} = \\mu + \\sum_{i=0}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j e_{t-j} + e_t\r\n\\] Here, \\(\\alpha\\) is an intercept term, \\(\\sum_{i=0}^{p} \\phi_i y_{t-i}\\) is the AR(p) process and \\(\\sum_{j=1}^{q} \\theta_j e_{t-j}\\) the MA(q) process. Their respective orders p and q can be derived either from examining the (partial) autocorrelations of the specific time series \\(y_t\\) or from the minimization of conventional information criteria like the Akaike (AIC) or the Bayesian (BIC).\r\nAn important prerequisite to forecasting with ARIMA is that the existence of stationarity in the data needs to be ensured. Stationary processes are mean-reverting. Their mean and variance remain more or less constant over time making it possible to apply standard results from statistics such as the law of large numbers and the central limit theorem. Contrary, when processes are non-stationary (textbook example: random walk), their mean and variance change over time. The stationarity property of a time series is tested through unit root tests which determine the order of integration I(d). For \\(d=0\\), the process is said to be stationary and for \\(d>0\\) it is non-stationary.\r\nIf a time series is characterized by an order of integration of one (which is the default case for stock price data), it is best to convert it to stationarity by first-difference for forecasting. In R, the whole process of order selection for the three ARIMA components, is done by the auto.arima() function. It makes use of a variation of the Hyndman and Khandakar (2008) algorithm which involves unit root tests, differencing, minimization of the AICc and maximum likelihood estimation. The general ARIMA(p,d,q) model with \\(d=1\\) to be estimated for stock price data then is:\r\n\\[\r\n\\hat{y_t} - y_{t-1} = \\mu + \\sum_{i=0}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j e_{t-j} + e_t\r\n\\] \\[\r\n\\hat{y_t} = \\mu + y_{t-1} + \\sum_{i=0}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j e_{t-j} + e_t\r\n\\] In this form, the ARIMA(p,1,q) model is ready to be applied to forecasting based on prior values of stock price data. The model produces point forecasts which are exhibited in the table of data. It also displays a 80% and a 95% confidence interval.\r\nProphet\r\n\r\nContrary to the ARIMA model, Prophet, the open source forecasting tool created by Facebook’s Data Science Team, employs an additive model where non-linear trends are fit with yearly, monthly, weekly and daily seasonality plus holiday effects. Note that ARIMA models also can be extended to SARIMA models by factoring in seasonality, although it is debatable if stock prices possess strong seasonal effects. Still, Prophet is an easy to deploy tool and widely used in forecasting. Its mathematical composition is as follows:\r\n\\[\r\ny(t) = g(t) + s(t) + h(t) +e(t)\r\n\\] where:\r\ng(t) represents the trend\r\ns(t) represents seasonal changes\r\nh(t) represents holiday effects\r\nand e(t) represents the error term\r\nThe fitting procedure does not require any data pre-processing like differencing to make it stationary. It estimates the whole range of historic stock closing price data accompanied by an 80% confidence interval and then forecasts values for the specified horizon.\r\nConclusion\r\nAlthough stock price forecasting remains a challenge, it can still benefit investment decision-making by helping to reduce uncertainty. The stock valuation app uses ARIMA models and Prophet for forecasting. These models are easy to deploy and established methods in the domain. However, the forecasting horizon is limited to a 90 day period since the models become too unreliable for longer term horizons. At a later date I want to patch the app according to the following:\r\nFix display bug\r\nAdd LSTM or another additional method to forecast\r\nForecast returns and volatility (GARCH)\r\nAdd accuracy metrics for Prophet\r\n\r\n\r\n\r\n",
    "preview": "posts/05.05.23_StockValuationPt.2/images/stock_app_pt2_1.png",
    "last_modified": "2023-05-05T11:13:44+02:00",
    "input_file": "05.05.23_StockValuationPt.2.knit.md"
  },
  {
    "path": "posts/05.05.23_StockValuationPt.1/",
    "title": "Stock Valuation via R Shiny, Part I: Chart Signals",
    "description": "The following R-Shiny-Series aims at showcasing an interactive tool for financial analysis in four parts, each dedicated to a key aspect of stock valuation. Through a mix of technical and fundamental analysis, I am specifically developing this web app to facilitate investment decision-making. This first part is concerned with chart signals.",
    "author": [
      {
        "name": "Dominic Prott",
        "url": {}
      }
    ],
    "date": "2023-03-20",
    "categories": [
      "Shiny",
      "Stocks",
      "R"
    ],
    "contents": "\r\nIntroduction\r\nThere is an abundance of investment applications and tools out there as new technologies make investing more accessible than ever before. I will add to this abundance by introducing you to my own simple version of a stock valuation app. Its purpose is to facilitate the valuation process of stocks and related investment decisions by mixing together parts of technical and fundamental analysis and displaying the results comprehensively on one platform. This first part deals with different ways of charting the closing stock price to draw conclusions on future price action. The next parts will then cover multi-step ahead forecasting of stock closing prices, discounted cash flow (DCF) analysis and earnings power value (EPV).\r\nThe following web application was created with Shiny, an R package which allows for a reactive user interface. Since it was not possible for me to embed the code in an interactive way in here, I will leave it at referring to screenshots and explaining the main features and the reasoning behind them merely through code snippets. Note that, for simplicity, I will refer to sketches instead of the whole code used. This blog entry will only consider the first tab in the sidebar, called “Chart”.\r\nR Shiny\r\nIn principle, shiny applications are structured into two parts: the user interface and the server. The user interface controls the layout and appearance of the app and the server gives it the functionality it needs to be interactive. In order to not extend this post too much, I will omit most of the server side in the following where the bulk of the code resides. Just know that it basically only involves defining a reactive function which is triggered by an input like pressing button and then handing it over to a render function where its output is transformed and styled for usage in the ui. The code chunk below demonstrates this interdependence between server and ui in a very rudimentary form. It creates an app that retrieves stock price data from yahoo finance with the help of the R package quantmod and then simply plots it:\r\n# First load libraries\r\nlibrary(shiny)\r\nlibrary(shinydashboard) \r\nlibrary(quantmod)\r\n\r\nshinyApp(\r\n  ui = dashboardPage(\r\n    dashboardHeader(title = \"Prototype\"),\r\n    dashboardSidebar(\r\n      #   Insert a text input widget\r\n      #   \"ticker\" defines the ID of this widget necessary to call it up later\r\n      #   \"Insert ticker symbol\" is what is displayed in the ui\r\n      #   through \"value = \" I make the default choice \"AMZN\"\r\n      \r\n      textInput(\"ticker\", \"Insert ticker symbol\", value = \"AMZN\"), \r\n      actionButton(\"button\", \"Compute\", class = \"btn-block\")\r\n    ),\r\n    dashboardBody(\r\n      plotOutput(\"stock_price\") #   the final output displayed in the main page\r\n    )\r\n  ),\r\n  server = function(input, output){\r\n    #   set up a reactive function to retrieve stock data from yahoo finance \r\n    #   base it on user's ticker input\r\n    \r\n    stock_price <- eventReactive(input$button, {\r\n              req(input$button, input$ticker)\r\n              tq_get(input$ticker, \r\n                     get  = \"stock.prices\",\r\n                     complete_cases = F) %>% \r\n                        select(date, close) %>% \r\n                        as.data.frame() \r\n      })\r\n    \r\n    output$stock_price <- renderPlot({\r\n      #   note that we call the retrieved data as a function of input in the plot\r\n      \r\n      plot(stock_price(), type = \"l\")\r\n      \r\n      })\r\n  }\r\n)\r\nThe basic recipe for the stock valuation tool then consists of adding more reactive features, applying different styling using boxes, tables and charts and making use of tab items with conditional panels. All the app does is essentially retrieve data like the one above and then transform and display it in a more useful way. First, I will briefly explain the ui and then explain its components.\r\nUI\r\nAgain, the user interface splits most shiny apps further into two integral parts: the sidebar, where users insert input into control widgets and the main body, where the input is transformed and then displayed. In my stock valuation app, the specific input widgets and their outputs are conditional on the tab the user has selected. The only exception to that is the ticker input as it stays in its place for easy access throughout usage. Using the same procedure to retrieve data on stock prices as above, historic data on any stock available at yahoo finance is downloaded into the app by selecting a ticker symbol and a time interval. The Amazon ticker symbol (= AMZN) is the default choice.\r\nAfter deciding on a stock, the next input decision naturally concerns the time frame. One can either pick the standard “1 Month” to “5 Years” intervals or select a specific date in the calender. The choice of the date interval affects not only the chart and table of data, but also the colored value boxes and the boxes with general information above. More on that later.\r\nBased on the retrieved data, the main output then is the chart. It can be represented as a simple line graph or a candlestick chart. Each representation can be modified further by the radio buttons.\r\n\r\nThe candlestick chart is often used to make inference on the behavioral patterns of the stock in technical analysis based on the display of all aspects of OHLC type data (trading volume included), whereas the simple line chart only displays closing prices.\r\nBoth options are plotted using the R package plotly. This allows for more interactivity, for example by zooming in and out or displaying single values at a given date in the chart. However, it does not get more sophisticated than that. Drawing lines manually for technical analysis as in trading platforms is not possible.\r\nThe group of radio buttons add to the chart in different ways. Although there are more sophisticated tools in the arsenal of a trader concerning trend analysis, I chose the “Stock Price”, “Simple Moving Average” (or SMA), “Bollinger Bands”, “Exponential Moving Average” (or EMA), “Moving Average Convergence-Divergence” (or MACD) and a combination of them to be displayed. They are easy to understand and work with and the specific values of the SMA and EMA are furthermore displayed in the data table.\r\nInfo Boxes\r\n\r\nThese four boxes are located above all other as they contain more general information. Like the ticker input they stay in their place after switching tabs. Technically, these boxes are also just boxes with information, not info Boxes in terms of R programming.\r\nEach box contains information on their most recent values respectively as well as absolute change and change in percent over the considered time interval. However, the last of the four boxes is an exception to that since it shows the 10-year government bond yield for the US in percent and its absolute and relative change is always related to the previous month.\r\nThe first of the boxes displays the most recent stock price of the chosen ticker. As for the following two boxes, the trading volume and its relative change in terms of the given time interval is showed in the footer. The second box contains information on the NASDAQ and the third on the DAX.\r\nThe boxes are put side-by-side by wrapping them in a fluidRow() in the ui. I created each box separately due to their separate content. However, the code is relatively similar. Representative for all four boxes, here is the code for the first one:\r\noutput$box1 = renderUI({\r\n      req(input$ticker, input$dateRange)\r\n      box(width = 3, #    width and height the same for each box\r\n          height = 12,\r\n          \r\n          #   Insert title which is reactive to the user's ticker symbol input\r\n          title = paste0(getQuote(paste(input$ticker, sep=\"\", collapse=\";\"), \r\n                                  what=yahooQF(c(\"Price/Sales\", \r\n                                                 \"P/E Ratio\",\r\n                                                 \"Price/EPS Estimate Next Year\",\r\n                                                 \"PEG Ratio\",\r\n                                                 \"Dividend Yield\", \r\n                                                 \"Market Capitalization\",\r\n                                                 \"Shares Outstanding\",\r\n                                                 \"Name\"))) %>% select(\"Name\")),\r\n          div(style=\"text-align: center;\", \r\n              h4(paste0(stock()$close %>% \r\n                          last() %>% \r\n                          round(digits = 2), \" USD\"))\r\n          ),\r\n          \r\n          div(style=\"text-align: center;\", \r\n              paste0(paste(ifelse(range1()$absolute >= 0, \"+\", \"-\"), \r\n                        round(abs(range1()$absolute), 2), sep = \"\"), \" (\", \r\n                     paste(ifelse(range1()$percent >= 0, \"+\", \"-\"), \r\n                        round(abs(range1()$percent), 2), sep = \"\"), \r\n                     \"%)\"\r\n              ),\r\n              style = ifelse(range1()$absolute < 0, \"color: red;\", \"color: green;\")\r\n          ),\r\n          \r\n          div(style=\"text-align: center;\", \r\n              paste0(input$dateRange, collapse = \" to \")\r\n          ),\r\n          \r\n          footer = div(style=\"text-align: right;\", \r\n                       paste0(\"Volume: \", last(stock()$volume), \" (\", \r\n                              paste(ifelse(range1()$volume >= 0, \"+\", \"-\"), \r\n                                 round(abs(range1()$volume), 2), sep = \"\"), \r\n                              \"%)\")\r\n          )\r\n      )\r\nI omit the reactive function as step 1 as the box is initiated without the press of a button. The renderUI({}) function is handed over directly from the server to the ui by calling it with uiOutput(\"box1\"). The box function heavily relies on the range1() function. It contains values of absolute and percentage change from the most recent data entry to the last of the time interval. The stock() is the basic function to retrieve historical stock data.\r\nValue Boxes\r\n\r\nI am yet not fully content with the value boxes, so they might be subject to change in the future. I used colors to indicate entry or exit signals, green and red respectively. Orange signals indecisiveness. However, the signals are more or less set at random. For example, to assess the risk of investing in the given stock I used the Yang-Zhang volatility estimator. This estimator makes use of OHLC data and gives out a specific value when rolling over all data from a given time interval minus 1 (nrow(data)-1). But which value will then be considered as “too high/too volatile”? I had a look at the most volatile stocks I could find for each date input interval and established a value of above 0.5 as too high and thus colored in red. Still, this value is not accurate and just set by observation of few different stocks.\r\nThe same problem applies to the P/E ratio. Which value is too high or too low? Here, I set all values above 20 as alarming. The P/E ratio is calculated from price per share divided by earnings per share. Thus, a high P/E ratio signals that either a company’s earnings per share are low relative to its price or its price is high relative to its earnings. The same is inversed for a low P/E ratio. However, the P/E ratio and the other indicators in the value boxes should be still used in combination with other indicators. As such, keep in mind that a high P/E ratio or risk factor might not be a red flag after all. I obtain the P/E ratio from yahoo. Sometimes it is not indicated there, so the value reverts to “NA”.\r\nThe “Reward” value box is easily explained as it just signals the mean of the daily rate of change for the given time interval. Negative values are marked red, positive ones green. The “RSI” values are displayed on a scale of zero to 100. The rule of thumb goes that stocks with values over 70 are considered “overbought” and thus expensive and in red. Stocks with values under 30 are considered “oversold” and thus relatively cheap and green. Values in between are in orange. It is calculated by setting in ratio average gains and average losses over a given time frame. Usually, the RSI is displayed as an oscillator or line graph. In the near future I want to add it this feature into the value box. But for now, it should be enough as an example:\r\noutput$rsi <- renderValueBox({\r\n\r\n      if(rsi() <= 30){\r\n        valueBox(\"RSI\", paste0(ceiling(rsi())),\r\n                 icon = icon(\"money-bill\"),\r\n                 color = \"green\")\r\n      } else if(rsi() > 30 && rsi() < 70){\r\n        valueBox(\"RSI\", paste0(ceiling(rsi())),\r\n                 icon = icon(\"exclamation-triangle\"),\r\n                 color = \"orange\")\r\n      } else if(rsi() >= 70){\r\n        valueBox(\"RSI\", paste0(ceiling(rsi())),\r\n                 icon = icon(\"ban\"),\r\n                 color = \"red\")\r\n      }\r\n      \r\n    })\r\nThe value boxes is deployed using valueBox() inside the renderValueBox({}) function, and is then handing its output over to the ui with valueBoxOutput(\"rsi\"). It draws from the previously created function rsi() which calculates rsi based on the historic stock price data and stores its most recent value.\r\nChart\r\nAs stated earlier, the chart of the stock price is either a line graph or a candlestick chart and can be modified with a group of radio buttons which facilitate trend analysis. The first option “Price” just displays the stock price. In candlestick mode, it also adds volume as a subplot.\r\n\r\nIn combination with the option “Chart”, the second option adds the simple moving average to the line graph. Although usually the 50-, 100- and 200-day moving averages are considered, here I use n=10 trading days as number of periods to average over. In the future, I might add options to refer to previous values as well as I want put more focus on long-term investing. The SMA smoothes out volatility and thus, it is useful to make underlying price trends more visible. Also, I want to add more options to combine different moving averages (or just different time frames of the same moving average) together in one graph as their relative behavior can give information about upcoming up- or downtrends.\r\n\r\n“Bollinger Bands” adds upper and lower bands two standard deviations +/- away from a 10-day SMA. Usually, most of the price fluctuations happen inside these bands. Thus, if a trading day breaks out of the bands, one can reasonably be certain that this is not a trend and will revert inside again in the next days. Bollinger Bands furthermore indicate whether stocks can be considered overbought or -sold when they continually touch the upper resistance or lower support line respectively. Price movements can then be anticipated accordingly.\r\n\r\nThe following exponential moving average is similar to the the SMA. Its purpose is to display price trends without noisy volatility. However, while the SMA is an unweighted moving average across the whole time frame, the EMA gives more weight to the most recent price data as it is considered more relevant. Here, it is set equally at n=10 averaging periods.\r\n\r\nThe moving average convergence-divergence (or MACD) is computed by subtracting a long-term EMA (here n=26 periods) from the short-term EMA (here n=12 periods). It is then combined with a signal line, a 9-day EMA. Their relationship acts as an indicator for buy or sell decisions. A buy signal arises when the MACD line crosses above the signal line and the reverse for a crossing below the signal line. MACD can be also set in ration with the oscillating RSI which can further give more security in buy or sell decisions. This feature might be added soon. Also the display of the MACD within the price chart is not very clean which also needs to be taken care of. The screenshot is a zoomed-in version.\r\n\r\nThe last option “All” combines the price chart with trading volume, SMA, EMA, MACD + Signal and daily return. In this way, the viewer gets a comprehensive overview over these indicators and their relationships.\r\n\r\nTable\r\nThe table of data displays all values for the given time interval. It shows the historic stock data in OHLC format plus columns for Volume, SMA and EMA. Through the use of the R package reactablefmtr, it is possible to sort each column with the default of the date column being arranged in descending order ranging from the most recent date to the last of the chosen time interval.\r\n\r\nConclusion\r\nIn this blog post, I explored the initial phase of the R shiny series where I introduced my interactive stock valuation tool. The primary objective was to demonstrate the tool’s capabilities for technical stock analysis, with a focus on trend analysis. Together with specifically selected indicators highlighted in value boxes, this web application offers a comprehensive overview over simple chart patterns, making it an easy-to-use assistant in stock valuation.\r\nScattered throughout this post I had some ideas on what to improve in the future. I would also be happy for your feedback and suggestions. In summary, my suggestions included:\r\nAdding a graph of historic RSI performance\r\nAdding an option to extend number of periods for SMA and EMA\r\nAdding an option to combine different moving averages in one graph\r\nMaking the MACD a subplot for better readability and introducing an option to combine it with other metrics in one graph\r\nPerhaps reworking the value boxes as a whole\r\nThe next part of the series will deal with forecasting stock prices using all available historic data on different estimation techniques like ARIMA, and Facebook’s Prophet.\r\n\r\n\r\n\r\n",
    "preview": "posts/05.05.23_StockValuationPt.1/images/stock_app_1.png",
    "last_modified": "2023-05-05T11:09:05+02:00",
    "input_file": {}
  }
]
